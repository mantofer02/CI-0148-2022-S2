{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbh9Ky3uYw8w"
      },
      "source": [
        "# Laboratorio 4 | Machine Learning\n",
        "* Marco Ferraro | B82957\n",
        "* Roy Padilla | B85854"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BIElRyehYw8y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # procesamiento de dataset\n",
        "import math # uso de función exp\n",
        "import numpy as np # uso de funciones y generación de arreglos\n",
        "import math # operación de raíz cuadrada\n",
        "\n",
        "LIMIT = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIojcHgKA8XJ",
        "outputId": "bbd96329-60a1-4bd0-a21b-3541af97a376"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1043, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "target_value_titanic = ['Survived_0', 'Survived_1']\n",
        "df_titanic = pd.read_csv('titanic.csv')\n",
        "df_titanic = df_titanic.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin']).dropna()\n",
        "\n",
        "# Se pasan a one-hot encoding las columnas 'Pclass','Sex' y 'Survived'\n",
        "df_titanic = pd.get_dummies(df_titanic,columns=['Embarked', 'Pclass','Sex', 'Survived'])\n",
        "y = df_titanic.loc[:, target_value_titanic]\n",
        "\n",
        "\n",
        "df_titanic = df_titanic.drop(columns = target_value_titanic)\n",
        "\n",
        "df_titanic.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p2NcJwCbQXb"
      },
      "source": [
        "1. Función `sigmoid(x)` que recibe un número `x` y retorna el resultado de la función sigmoide para dicho `x:   1/(1 + e^(-x))`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tZw1OSmmbVeA"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  try:\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpO9tpHddsST"
      },
      "source": [
        "2. Función d_sigmoid(y) que recibe un número y (tal que y = sigmoid(x)) y retorna el resultado de la derivada de la función sigmoide para el y dado: y*(1-y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nDMkouAabg_t"
      },
      "outputs": [],
      "source": [
        "def d_sigmoid(y):\n",
        "  try:\n",
        "    return y * (1 - y)\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnfH9hrTYw82"
      },
      "source": [
        "3. Función `tanh(x)` que recibe un número x y retorna el resultado de la función tangente hiperbólico para dicho `x: (e^x - e^(-x))/(e^x + e^(-x))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "des8zb9AYw82"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "  try:\n",
        "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z1ee3m8Yw82"
      },
      "source": [
        "4. Función `d_tanh(y)` que recibe un número y (tal que `y = tanh(x)`) y retorna el resultado de la derivada de la función tangente hiperbólico para el `y` dado: `1 - y^2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OK01MHU4Yw83"
      },
      "outputs": [],
      "source": [
        "def d_tanh(y):\n",
        "  try:\n",
        "    return 1-y**2\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZCXSZ1VYw83"
      },
      "source": [
        "5. Función `relu(x)` que recibe un número x y retorna el resultado de la función lineal rectificada para dicho `x: (x if x > 0 else 0)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2qaUkgxhYw83"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  try:\n",
        "    return x if x > 0 else 0\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9xa3XwYYw84"
      },
      "source": [
        "6. Función `d_relu(y)` que recibe un número `y` (tal que `y = relu(x)`) y retorna el resultado de la derivada de la función lineal rectificada para el y dado: `1 if y >0 else 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5EPGLc9lYw84"
      },
      "outputs": [],
      "source": [
        "def d_relu(y):\n",
        "  try:\n",
        "    return 1 if y > 0 else 0\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v29u907lYw85"
      },
      "source": [
        "7. `Función lrelu(x)` que recibe un número x y retorna el resultado de la función lineal rectificada con fuga para dicho x: (x if x>0 else 0.01x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VpnfjckGYw86"
      },
      "outputs": [],
      "source": [
        "def lrelu(x):\n",
        "  try:\n",
        "    return x if x>0 else 0.01*x\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jte2MmtYw86"
      },
      "source": [
        "8. Función `d_lrelu(y)` que recibe un número y (tal que y = lrelu(x)) y retorna el resultado de la derivada de la función lineal rectificada con fuga para el y dado: 1 if y>0 else 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fuTZlv9fYw86"
      },
      "outputs": [],
      "source": [
        "def d_lrelu(y):\n",
        "  try:\n",
        "    return 1 if y > 0 else 0.01\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzUsUYKfYw86"
      },
      "source": [
        "`Clase DenseNN`\n",
        "\n",
        "\n",
        "10. `Método predict(self, x)` que recibe una matriz de datos x de tamaño nxm, donde m coincide con el primer valor de layers al construir la red neuronal. Este método ejecuta el forward propagation aplicando la multiplicación con la matrices de pesos y las funciones de activación de cada capa. Finalmente, retorna una matriz de datos nxp, donde p coincide con el último de layers y corresponde a los valores predichos por la red neuronal para los casos x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7UrHMF-hYw87"
      },
      "outputs": [],
      "source": [
        "class DenseNN:\n",
        "    def __init__(self, layers, activation, seed = 0):\n",
        "      np.random.seed(seed)\n",
        "\n",
        "      self.x_count = layers[0]\n",
        "      self.y_count = layers[-1]\n",
        "      self.activation = activation\n",
        "      self.epoch = 0\n",
        "      self.momentum = 0\n",
        "      self.lr = 0\n",
        "      self.decay = 0\n",
        "\n",
        "      self.weight_matrixs = []\n",
        "      self.delta_weight_matrix = []\n",
        "      \n",
        "      self.net_values = []\n",
        "      self.activation_values = []\n",
        "      self.layers_errors = []\n",
        "\n",
        "      self.activation_values_reduced = []\n",
        "      self.layers_errors_reduced = []\n",
        "\n",
        "      self.error_history = []\n",
        "\n",
        "      # inicialización de la matriz de pesos y bias\n",
        "      for index in range(len(layers[1:])):\n",
        "        fan_in = layers[index] # capa anterior (cantidad de columnas)\n",
        "        fan_out = 0 if len(layers[1:-1]) < index + 1 else layers[index+2] # capa siguiente\n",
        "        self.weight_matrixs.append(np.random.normal(loc = 0, scale=2.0/math.sqrt(fan_in + fan_out), size = (fan_in + 1, layers[index + 1])))\n",
        "\n",
        "    def predict(self, x = pd.DataFrame):\n",
        "      x = x.assign(bias = np.ones(x.shape[0])) # inserción del bias\n",
        "      predictions = np.array([])\n",
        "      for index, row in x.iterrows():\n",
        "        # llamada al forward propagation para la fila\n",
        "        predictions = np.append(predictions, self._fowardPropagation(row))\n",
        "      return predictions \n",
        "\n",
        "    def train(self, lr=0.05, momentum=0, decay=0):\n",
        "      self.epoch = 0\n",
        "      self.error_history = []\n",
        "      # REVISAR\n",
        "      self.lr = lr\n",
        "      self.momentum = momentum\n",
        "      self.decay = decay\n",
        "      self.delta_weight_matrix = []\n",
        "      for weight_matrix in self.weight_matrixs:\n",
        "        self.delta_weight_matrix.append(np.copy(weight_matrix))\n",
        "\n",
        "    def _fowardPropagation(self, row):\n",
        "      # limpieza de los valores netos, activación y error\n",
        "      # self.activation_values = []\n",
        "      self.net_values = []\n",
        "      \n",
        "      # inicialización de los valores para la primer capa oculta\n",
        "      net_k = np.matmul(np.transpose(row.to_numpy()), self.weight_matrixs[0])\n",
        "      o_k = self._getActivationValue(0, net_k)\n",
        "      \n",
        "      self.net_values.append(net_k)\n",
        "      \n",
        "      # recorrido desde la segunda capa oculta hasta la salida\n",
        "      for weight_index in range(len(self.weight_matrixs[1:])):\n",
        "        # aplicación de la multiplicación: x*w\n",
        "        net_k = np.matmul(np.transpose(np.append(o_k, 1)), self.weight_matrixs[weight_index + 1])\n",
        "        # Aplicación de la función de activación en cada uno de los resultados\n",
        "        o_k = self._getActivationValue(weight_index + 1, net_k)\n",
        "        self.net_values.append(net_k)\n",
        "\n",
        "      # Retorna el valor final de la predicción de la tupla\n",
        "      return o_k\n",
        "\n",
        "    def backPropagation(self, x, y):\n",
        "      x = x.assign(bias = np.ones(x.shape[0]))\n",
        "\n",
        "      self.layers_errors = [np.array([])] * len(self.weight_matrixs)\n",
        "      self.activation_values = [np.array([])] * len(self.weight_matrixs)\n",
        "\n",
        "      self.layers_errors_reduced = [np.array([])] * len(self.weight_matrixs)\n",
        "      self.activation_values_reduced = [np.array([])] * len(self.weight_matrixs)\n",
        "\n",
        "      row_counter = 1\n",
        "      for index, row in x.iterrows():\n",
        "        y_predict = self._fowardPropagation(row)\n",
        "        y_true = y.loc[index]\n",
        "        dj = []\n",
        "        for layer_index in reversed(range(len(self.weight_matrixs))):\n",
        "          if layer_index == len(self.weight_matrixs) - 1: # caso para capa de salida\n",
        "            dj = 2 * (y_predict - y_true) * self._getDerivativeActivationValues(layer_index, self.net_values[layer_index])\n",
        "            oj = self._getActivationValue(layer_index, self.net_values[layer_index])\n",
        "\n",
        "          elif layer_index == 0: # caso para capa de entrada\n",
        "            dj = (self.weight_matrixs[layer_index + 1] @ dj) * np.append(self._getDerivativeActivationValues(layer_index, self.net_values[layer_index]), 1)\n",
        "            dj = np.delete(dj, -1) # eliminación del bias\n",
        "            oj = self.net_values[layer_index]\n",
        "\n",
        "          else: # caso para capas ocultas\n",
        "            dj = (self.weight_matrixs[layer_index + 1] @ dj) * np.append(self._getDerivativeActivationValues(layer_index, self.net_values[layer_index]), 1)\n",
        "            dj = np.delete(dj, -1) # eliminación del bias\n",
        "            oj = self._getActivationValue(layer_index, self.net_values[layer_index])\n",
        "          # Generación de matriz de errores\n",
        "          self.layers_errors[layer_index] = np.append(self.layers_errors[layer_index], np.array(dj))\n",
        "          self.layers_errors[layer_index] = np.reshape(self.layers_errors[layer_index], (row_counter,dj.shape[0]))\n",
        "          # Generación de matriz de oj (valores de activación)\n",
        "          self.activation_values[layer_index] = np.append(self.activation_values[layer_index], np.array(oj))\n",
        "          self.activation_values[layer_index] = np.reshape(self.activation_values[layer_index], (row_counter,oj.shape[0]))\n",
        "        # Incremento del row index para el reshape\n",
        "        row_counter += 1\n",
        "      \n",
        "\n",
        "      # Reducir dimensionalidad\n",
        "      for layer_index in range(len(self.weight_matrixs)):\n",
        "        for col in range(len(self.layers_errors[layer_index][0])):\n",
        "          error_mean = np.mean(self.layers_errors[layer_index][:, col])\n",
        "          activaction_mean = np.mean(self.activation_values[layer_index][:, col])\n",
        "\n",
        "          self.layers_errors_reduced[layer_index] = np.append(self.layers_errors_reduced[layer_index], error_mean)\n",
        "          self.activation_values_reduced[layer_index] = np.append(self.activation_values_reduced[layer_index], activaction_mean)\n",
        "\n",
        "      # apply values to delta w\n",
        "      for layer_index in reversed(range(len(self.weight_matrixs))):\n",
        "          if layer_index != 0:\n",
        "            activation_vec = np.array([self.activation_values_reduced[layer_index - 1]])\n",
        "            errors_vec = np.array([self.layers_errors_reduced[layer_index]])\n",
        "            result = activation_vec.T @ errors_vec\n",
        "\n",
        "            for i in range(len(result)):\n",
        "              for j in range(len(result[0])):\n",
        "                self.weight_matrixs[layer_index][i][j] = result[i][j]\n",
        "\n",
        "      self.error_history.append(self.layers_errors_reduced)\n",
        "\n",
        "    def step(self):      \n",
        "      for layer_index in range(len(self.weight_matrixs)):\n",
        "        self.weight_matrixs[layer_index] = self.weight_matrixs[layer_index] - (self.lr *  self.delta_weight_matrix[layer_index])\n",
        "\n",
        "      self.epoch += 1\n",
        "\n",
        "    def _getActivationValue(self, layer_number, values):\n",
        "      return_values = []\n",
        "      for value in np.nditer(values):\n",
        "        return_values.append(self._applyActivation(layer_number, value))\n",
        "      return np.array(return_values)\n",
        "\n",
        "    def _getDerivativeActivationValues(self, layer_number, values):\n",
        "      return_values = []\n",
        "      for value in np.nditer(values):\n",
        "        return_values.append(self._applyActivationDerivative(layer_number, value))\n",
        "      return np.array(return_values)\n",
        "\n",
        "    def _applyActivation(self, layer_number, value):\n",
        "      if self.activation[layer_number] == 's': # aplicar función sigmoide\n",
        "        return sigmoid(value)\n",
        "      elif self.activation[layer_number] == 't': # aplicar función tangente hiperbólico\n",
        "        return tanh(value)\n",
        "      elif self.activation[layer_number] == 'r': # aplicar función relu\n",
        "        return relu(value)\n",
        "      elif self.activation[layer_number] == 'l': # aplicar función leaky relu\n",
        "        return lrelu(value)\n",
        "      return 0\n",
        "\n",
        "    def _applyActivationDerivative(self, layer_number, value):\n",
        "      if self.activation[layer_number] == 's': # aplicar función sigmoide\n",
        "        return d_sigmoid(value)\n",
        "      elif self.activation[layer_number] == 't': # aplicar función tangente hiperbólico\n",
        "        return d_tanh(value)\n",
        "      elif self.activation[layer_number] == 'r': # aplicar función relu\n",
        "        return d_relu(value)\n",
        "      elif self.activation[layer_number] == 'l': # aplicar función leaky relu\n",
        "        return d_lrelu(value)\n",
        "      return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BeH4MfoMAiR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869ceebf-d231-479e-dbce-e5c5badaa8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[array([67881.95036782,  8727.06990055, 96935.93909135]),\n",
              "  array([ 0.09773937, 25.3554038 ])],\n",
              " [array([8.09142819e+12, 1.50457269e+11, 1.30422955e+13]),\n",
              "  array([-4.88343729e+01, -6.33775847e+06])],\n",
              " [array([9.50042867e+28, 2.04561923e+27, 3.15343733e+29]),\n",
              "  array([2.12548090e+07, 2.36249239e+17])],\n",
              " [array([3.81351709e+60, 7.02666302e+58, 6.15293043e+60]),\n",
              "  array([-2.38403836e+18, -4.46158171e+38])],\n",
              " [array([1.87078781e+124, 4.00132141e+122, 6.21862322e+124]),\n",
              "  array([4.05138667e+40, 9.36611762e+80])]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "neural_net = DenseNN([12, 3, 2],['s','s'], 0)\n",
        "neural_net.train()\n",
        "\n",
        "for i in range(5):\n",
        "  neural_net.backPropagation(df_titanic, y)\n",
        "  neural_net.step()\n",
        "\n",
        "neural_net.error_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16RGi5e8la20",
        "outputId": "f75cac19-750c-4390-b295-74c374797a5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[array([ -45.48425443, -213.65487753,  293.51226294]),\n",
              "  array([130.30155939,   0.        ])],\n",
              " [array([4.04836017e+09, 1.11022896e+09, 4.60653907e+09]),\n",
              "  array([8.27375571e+05, 5.15002755e+00])],\n",
              " [array([1.39557863e+17, 3.82703765e+16, 1.58803569e+17]),\n",
              "  array([4.72827495e+09, 2.94360907e+04])],\n",
              " [array([3.86339038e+24, 1.05944158e+24, 4.39617067e+24]),\n",
              "  array([2.41767586e+13, 1.50513514e+08])],\n",
              " [array([8.47975276e+31, 2.32536756e+31, 9.64915182e+31]),\n",
              "  array([1.09885630e+17, 6.84098006e+11])]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "neural_net = DenseNN([12, 3, 2],['r','r'], 0)\n",
        "neural_net.train()\n",
        "\n",
        "for i in range(5):\n",
        "  neural_net.backPropagation(df_titanic, y)\n",
        "  neural_net.step()\n",
        "\n",
        "neural_net.error_history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neural_net = DenseNN([12, 5, 6, 2],['s', 's','r'], 0)\n",
        "neural_net.train()\n",
        "\n",
        "for i in range(5):\n",
        "  neural_net.backPropagation(df_titanic, y)\n",
        "  neural_net.step()\n",
        "\n",
        "neural_net.error_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJqvDuSlnzPs",
        "outputId": "7e50149c-74b3-403f-d0c7-f830d011e74b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[array([  30195.28482588, -105934.84230162,    1098.62033618,\n",
              "          158568.84708647,   32451.2614224 ]),\n",
              "  array([  7.89793111,   1.05351558, -68.63848912,   0.47804373,\n",
              "          -0.80754907,  -0.40975193]),\n",
              "  array([7.23971936, 0.        ])],\n",
              " [array([-1.84131644e+16, -3.97520538e+16, -8.51066558e+14, -3.02209951e+16,\n",
              "         -4.18625437e+15]),\n",
              "  array([-4.83148266e+07, -1.32027837e+06, -9.29231636e+09, -2.31204865e+05,\n",
              "         -9.59848084e+05, -2.49774470e+05]),\n",
              "  array([22.37232709,  0.        ])],\n",
              " [array([3.09250512e+37, 8.39571644e+37, 1.25172445e+36, 5.02080964e+37,\n",
              "         7.15086140e+36]),\n",
              "  array([-9.68633035e+20, -7.20267370e+17,  1.26766487e+23, -2.21573361e+16,\n",
              "          1.03029743e+15,  4.26963283e+13]),\n",
              "  array([1.60299211, 0.        ])],\n",
              " [array([-4.15068424e+76, -1.23559938e+77, -1.56643334e+75, -6.70528208e+76,\n",
              "         -9.68035760e+75]),\n",
              "  array([-5.79437839e+44,  2.30443817e+38,  1.43118764e+49, -1.68149690e+35,\n",
              "          7.20135168e+32,  7.56844576e+29]),\n",
              "  array([1.09347819, 0.        ])],\n",
              " [array([3.61844300e+156, 8.16040427e+156, 1.62575935e+155, 5.93301991e+156,\n",
              "         8.26087382e+155]),\n",
              "  array([-1.28400801e+093,  1.46074170e+080, -1.44379075e+103,\n",
              "         -5.99679012e+073, -3.72358324e+070, -4.20625701e+064]),\n",
              "  array([7.41924025, 0.        ])]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Se va reduciendo el error? Intente diferentes combinaciones de parámetros, arquitecturas y funciones de activación para lograr que su algoritmo converja. \n",
        "\n",
        "¿Cuál combinación le produjo el mejor resultado?\n",
        "\n",
        "> Como se puede ver en los resultado, los valores de los errores de las neuronas van covergiendo pero para constantes muy altas. Inclusive, para las funciones. Esto se debe a la implementación de limites que se está manejando dentro de las funciones matemáticas. Consideramos que, entre varias configuraciones, la que maneja mejor consistencia es la úlitma que maneja `DenseNN([12, 5, 6, 2],['s', 's','r'], 0)`\n",
        "\n",
        "¿Qué aprendió de estos experimentos? (Se vale no haber aprendido nada, venimos a\n",
        "experimentar)\n",
        "\n",
        "> Dentro de estos experimentos se tuvo un entendimiento más amplio sobre el uso del gradiente del descenso. Además, se aprendió de como se debe mantener un buen registro y cálculo del error para el cálculo de backpropagation.\n",
        "\n"
      ],
      "metadata": {
        "id": "hoFrcuZPoOxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jupyter nbconvert --to html /PATH/TO/YOUR/NOTEBOOKFILE.ipynb"
      ],
      "metadata": {
        "id": "LUnZot9ntUKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}