{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbh9Ky3uYw8w"
      },
      "source": [
        "# Laboratorio 4 | Machine Learning\n",
        "* Marco Ferraro | B82957\n",
        "* Roy Padilla | B85854"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIElRyehYw8y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # procesamiento de dataset\n",
        "import math # uso de función exp\n",
        "import numpy as np # uso de funciones y generación de arreglos\n",
        "import math # operación de raíz cuadrada\n",
        "\n",
        "LIMIT = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIojcHgKA8XJ",
        "outputId": "bbd96329-60a1-4bd0-a21b-3541af97a376"
      },
      "outputs": [],
      "source": [
        "target_value_titanic = ['Survived_0', 'Survived_1']\n",
        "df_titanic = pd.read_csv('titanic.csv')\n",
        "df_titanic = df_titanic.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin']).dropna()\n",
        "\n",
        "# Se pasan a one-hot encoding las columnas 'Pclass','Sex' y 'Survived'\n",
        "df_titanic = pd.get_dummies(df_titanic,columns=['Embarked', 'Pclass','Sex', 'Survived'])\n",
        "y = df_titanic.loc[:, target_value_titanic]\n",
        "\n",
        "\n",
        "df_titanic = df_titanic.drop(columns = target_value_titanic)\n",
        "\n",
        "df_titanic.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p2NcJwCbQXb"
      },
      "source": [
        "1. Función `sigmoid(x)` que recibe un número `x` y retorna el resultado de la función sigmoide para dicho `x:   1/(1 + e^(-x))`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZw1OSmmbVeA"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  try:\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpO9tpHddsST"
      },
      "source": [
        "2. Función d_sigmoid(y) que recibe un número y (tal que y = sigmoid(x)) y retorna el resultado de la derivada de la función sigmoide para el y dado: y*(1-y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDMkouAabg_t"
      },
      "outputs": [],
      "source": [
        "def d_sigmoid(y):\n",
        "  try:\n",
        "    return y * (1 - y)\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnfH9hrTYw82"
      },
      "source": [
        "3. Función `tanh(x)` que recibe un número x y retorna el resultado de la función tangente hiperbólico para dicho `x: (e^x - e^(-x))/(e^x + e^(-x))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "des8zb9AYw82"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "  try:\n",
        "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z1ee3m8Yw82"
      },
      "source": [
        "4. Función `d_tanh(y)` que recibe un número y (tal que `y = tanh(x)`) y retorna el resultado de la derivada de la función tangente hiperbólico para el `y` dado: `1 - y^2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK01MHU4Yw83"
      },
      "outputs": [],
      "source": [
        "def d_tanh(y):\n",
        "  try:\n",
        "    return 1-y**2\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZCXSZ1VYw83"
      },
      "source": [
        "5. Función `relu(x)` que recibe un número x y retorna el resultado de la función lineal rectificada para dicho `x: (x if x > 0 else 0)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qaUkgxhYw83"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  try:\n",
        "    return x if x > 0 else 0\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9xa3XwYYw84"
      },
      "source": [
        "6. Función `d_relu(y)` que recibe un número `y` (tal que `y = relu(x)`) y retorna el resultado de la derivada de la función lineal rectificada para el y dado: `1 if y >0 else 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EPGLc9lYw84"
      },
      "outputs": [],
      "source": [
        "def d_relu(y):\n",
        "  try:\n",
        "    return 1 if y > 0 else 0\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v29u907lYw85"
      },
      "source": [
        "7. `Función lrelu(x)` que recibe un número x y retorna el resultado de la función lineal rectificada con fuga para dicho x: (x if x>0 else 0.01x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpnfjckGYw86"
      },
      "outputs": [],
      "source": [
        "def lrelu(x):\n",
        "  try:\n",
        "    return x if x>0 else 0.01*x\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jte2MmtYw86"
      },
      "source": [
        "8. Función `d_lrelu(y)` que recibe un número y (tal que y = lrelu(x)) y retorna el resultado de la derivada de la función lineal rectificada con fuga para el y dado: 1 if y>0 else 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuTZlv9fYw86"
      },
      "outputs": [],
      "source": [
        "def d_lrelu(y):\n",
        "  try:\n",
        "    return 1 if y > 0 else 0.01\n",
        "  except OverflowError:\n",
        "    return LIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzUsUYKfYw86"
      },
      "source": [
        "`Clase DenseNN`\n",
        "\n",
        "\n",
        "10. `Método predict(self, x)` que recibe una matriz de datos x de tamaño nxm, donde m coincide con el primer valor de layers al construir la red neuronal. Este método ejecuta el forward propagation aplicando la multiplicación con la matrices de pesos y las funciones de activación de cada capa. Finalmente, retorna una matriz de datos nxp, donde p coincide con el último de layers y corresponde a los valores predichos por la red neuronal para los casos x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UrHMF-hYw87"
      },
      "outputs": [],
      "source": [
        "class DenseNN:\n",
        "    def __init__(self, layers, activation, seed = 0):\n",
        "      np.random.seed(seed)\n",
        "\n",
        "      self.x_count = layers[0]\n",
        "      self.y_count = layers[-1]\n",
        "      self.activation = activation\n",
        "      self.epoch = 0\n",
        "      self.momentum = 0\n",
        "      self.lr = 0\n",
        "      self.decay = 0\n",
        "\n",
        "      self.weight_matrixs = []\n",
        "      self.delta_weight_matrix = []\n",
        "      self.prev_delta_weight_matrix = []\n",
        "      \n",
        "      self.net_values = []\n",
        "      self.activation_values = []\n",
        "      self.layers_errors = []\n",
        "\n",
        "      self.activation_values_reduced = []\n",
        "      self.layers_errors_reduced = []\n",
        "\n",
        "      self.error_history = []\n",
        "\n",
        "      # inicialización de la matriz de pesos y bias\n",
        "      for index in range(len(layers[1:])):\n",
        "        fan_in = layers[index] # capa anterior (cantidad de columnas)\n",
        "        fan_out = 0 if len(layers[1:-1]) < index + 1 else layers[index+2] # capa siguiente\n",
        "        self.weight_matrixs.append(np.random.normal(loc = 0, scale=2.0/math.sqrt(fan_in + fan_out), size = (fan_in + 1, layers[index + 1])))\n",
        "\n",
        "    def predict(self, x = pd.DataFrame):\n",
        "      x = x.assign(bias = np.ones(x.shape[0])) # inserción del bias\n",
        "      predictions = np.array([])\n",
        "      for index, row in x.iterrows():\n",
        "        # llamada al forward propagation para la fila\n",
        "        predictions = np.append(predictions, self._fowardPropagation(row))\n",
        "      return predictions \n",
        "\n",
        "    def train(self, lr=0.05, momentum=0, decay=0):\n",
        "      self.epoch = 0\n",
        "      self.error_history = []\n",
        "      # REVISAR\n",
        "      self.lr = lr\n",
        "      self.momentum = momentum\n",
        "      self.decay = decay\n",
        "      self.delta_weight_matrix = []\n",
        "      for weight_matrix in self.weight_matrixs:\n",
        "        self.delta_weight_matrix.append(np.copy(weight_matrix))\n",
        "        self.prev_delta_weight_matrix.append(np.copy(weight_matrix))\n",
        "\n",
        "\n",
        "\n",
        "    def _fowardPropagation(self, row):\n",
        "      # limpieza de los valores netos, activación y error\n",
        "      # self.activation_values = []\n",
        "      self.net_values = []\n",
        "      \n",
        "      # inicialización de los valores para la primer capa oculta\n",
        "      net_k = np.matmul(np.transpose(row.to_numpy()), self.weight_matrixs[0])\n",
        "      o_k = self._getActivationValue(0, net_k)\n",
        "      \n",
        "      self.net_values.append(net_k)\n",
        "      \n",
        "      # recorrido desde la segunda capa oculta hasta la salida\n",
        "      for weight_index in range(len(self.weight_matrixs[1:])):\n",
        "        # aplicación de la multiplicación: x*w\n",
        "        net_k = np.matmul(np.transpose(np.append(o_k, 1)), self.weight_matrixs[weight_index + 1])\n",
        "        # Aplicación de la función de activación en cada uno de los resultados\n",
        "        o_k = self._getActivationValue(weight_index + 1, net_k)\n",
        "        self.net_values.append(net_k)\n",
        "\n",
        "      # Retorna el valor final de la predicción de la tupla\n",
        "      return o_k\n",
        "\n",
        "    def backPropagation(self, x, y):\n",
        "      x = x.assign(bias = np.ones(x.shape[0]))\n",
        "\n",
        "      self.layers_errors = [np.array([])] * len(self.weight_matrixs)\n",
        "      self.activation_values = [np.array([])] * len(self.weight_matrixs)\n",
        "\n",
        "      self.layers_errors_reduced = [np.array([])] * len(self.weight_matrixs)\n",
        "      self.activation_values_reduced = [np.array([])] * len(self.weight_matrixs)\n",
        "\n",
        "      row_counter = 1\n",
        "\n",
        "      for weight_matrix in self.delta_weight_matrix:\n",
        "        self.prev_delta_weight_matrix.append(np.copy(weight_matrix))\n",
        "\n",
        "      for index, row in x.iterrows():\n",
        "        y_predict = self._fowardPropagation(row)\n",
        "        y_true = y.loc[index]\n",
        "        dj = []\n",
        "        for layer_index in reversed(range(len(self.weight_matrixs))):\n",
        "          if layer_index == len(self.weight_matrixs) - 1: # caso para capa de salida\n",
        "            dj = 2 * (y_predict - y_true) * self._getDerivativeActivationValues(layer_index, self.net_values[layer_index])\n",
        "            oj = self._getActivationValue(layer_index, self.net_values[layer_index])\n",
        "\n",
        "          elif layer_index == 0: # caso para capa de entrada\n",
        "            dj = (self.weight_matrixs[layer_index + 1] @ dj) * np.append(self._getDerivativeActivationValues(layer_index, self.net_values[layer_index]), 1)\n",
        "            dj = np.delete(dj, -1) # eliminación del bias\n",
        "            oj = self.net_values[layer_index]\n",
        "\n",
        "          else: # caso para capas ocultas\n",
        "            dj = (self.weight_matrixs[layer_index + 1] @ dj) * np.append(self._getDerivativeActivationValues(layer_index, self.net_values[layer_index]), 1)\n",
        "            dj = np.delete(dj, -1) # eliminación del bias\n",
        "            oj = self._getActivationValue(layer_index, self.net_values[layer_index])\n",
        "          # Generación de matriz de errores\n",
        "          self.layers_errors[layer_index] = np.append(self.layers_errors[layer_index], np.array(dj))\n",
        "          self.layers_errors[layer_index] = np.reshape(self.layers_errors[layer_index], (row_counter,dj.shape[0]))\n",
        "          # Generación de matriz de oj (valores de activación)\n",
        "          self.activation_values[layer_index] = np.append(self.activation_values[layer_index], np.array(oj))\n",
        "          self.activation_values[layer_index] = np.reshape(self.activation_values[layer_index], (row_counter,oj.shape[0]))\n",
        "        # Incremento del row index para el reshape\n",
        "        row_counter += 1\n",
        "      \n",
        "\n",
        "      # Reducir dimensionalidad\n",
        "      for layer_index in range(len(self.weight_matrixs)):\n",
        "        for col in range(len(self.layers_errors[layer_index][0])):\n",
        "          error_mean = np.mean(self.layers_errors[layer_index][:, col])\n",
        "          activaction_mean = np.mean(self.activation_values[layer_index][:, col])\n",
        "\n",
        "          self.layers_errors_reduced[layer_index] = np.append(self.layers_errors_reduced[layer_index], error_mean)\n",
        "          self.activation_values_reduced[layer_index] = np.append(self.activation_values_reduced[layer_index], activaction_mean)\n",
        "\n",
        "      # apply values to delta w\n",
        "      for layer_index in reversed(range(len(self.weight_matrixs))):\n",
        "          if layer_index != 0:\n",
        "            activation_vec = np.array([self.activation_values_reduced[layer_index - 1]])\n",
        "            errors_vec = np.array([self.layers_errors_reduced[layer_index]])\n",
        "            result = activation_vec.T @ errors_vec\n",
        "\n",
        "            for i in range(len(result)):\n",
        "              for j in range(len(result[0])):\n",
        "                self.weight_matrixs[layer_index][i][j] = result[i][j]\n",
        "\n",
        "      self.error_history.append(self.layers_errors_reduced)\n",
        "\n",
        "    def step(self):\n",
        "      for layer_index in range(len(self.weight_matrixs)):\n",
        "        self.weight_matrixs[layer_index] = self.weight_matrixs[layer_index] - (self.lr *  (self.delta_weight_matrix[layer_index] + self.momentum * self.prev_delta_weight_matrix[layer_index]))\n",
        "\n",
        "      self.lr = self.lr / (1 + self.decay)\n",
        "      self.epoch += 1\n",
        "\n",
        "    def _getActivationValue(self, layer_number, values):\n",
        "      return_values = []\n",
        "      for value in np.nditer(values):\n",
        "        return_values.append(self._applyActivation(layer_number, value))\n",
        "      return np.array(return_values)\n",
        "\n",
        "    def _getDerivativeActivationValues(self, layer_number, values):\n",
        "      return_values = []\n",
        "      for value in np.nditer(values):\n",
        "        return_values.append(self._applyActivationDerivative(layer_number, value))\n",
        "      return np.array(return_values)\n",
        "\n",
        "    def _applyActivation(self, layer_number, value):\n",
        "      if self.activation[layer_number] == 's': # aplicar función sigmoide\n",
        "        return sigmoid(value)\n",
        "      elif self.activation[layer_number] == 't': # aplicar función tangente hiperbólico\n",
        "        return tanh(value)\n",
        "      elif self.activation[layer_number] == 'r': # aplicar función relu\n",
        "        return relu(value)\n",
        "      elif self.activation[layer_number] == 'l': # aplicar función leaky relu\n",
        "        return lrelu(value)\n",
        "      return 0\n",
        "\n",
        "    def _applyActivationDerivative(self, layer_number, value):\n",
        "      if self.activation[layer_number] == 's': # aplicar función sigmoide\n",
        "        return d_sigmoid(value)\n",
        "      elif self.activation[layer_number] == 't': # aplicar función tangente hiperbólico\n",
        "        return d_tanh(value)\n",
        "      elif self.activation[layer_number] == 'r': # aplicar función relu\n",
        "        return d_relu(value)\n",
        "      elif self.activation[layer_number] == 'l': # aplicar función leaky relu\n",
        "        return d_lrelu(value)\n",
        "      return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeH4MfoMAiR7",
        "outputId": "869ceebf-d231-479e-dbce-e5c5badaa8bc"
      },
      "outputs": [],
      "source": [
        "neural_net = DenseNN([12, 3, 2],['s','s'], 0)\n",
        "neural_net.train()\n",
        "\n",
        "for i in range(5):\n",
        "  neural_net.backPropagation(df_titanic, y)\n",
        "  neural_net.step()\n",
        "\n",
        "neural_net.error_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16RGi5e8la20",
        "outputId": "f75cac19-750c-4390-b295-74c374797a5d"
      },
      "outputs": [],
      "source": [
        "neural_net = DenseNN([12, 3, 2],['r','r'], 0)\n",
        "neural_net.train()\n",
        "\n",
        "for i in range(5):\n",
        "  neural_net.backPropagation(df_titanic, y)\n",
        "  neural_net.step()\n",
        "\n",
        "neural_net.error_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJqvDuSlnzPs",
        "outputId": "7e50149c-74b3-403f-d0c7-f830d011e74b"
      },
      "outputs": [],
      "source": [
        "neural_net = DenseNN([12, 5, 6, 2],['s', 's','r'], 0)\n",
        "neural_net.train()\n",
        "\n",
        "for i in range(5):\n",
        "  neural_net.backPropagation(df_titanic, y)\n",
        "  neural_net.step()\n",
        "\n",
        "neural_net.error_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFrcuZPoOxy"
      },
      "source": [
        "¿Se va reduciendo el error? Intente diferentes combinaciones de parámetros, arquitecturas y funciones de activación para lograr que su algoritmo converja. \n",
        "\n",
        "¿Cuál combinación le produjo el mejor resultado?\n",
        "\n",
        "> Como se puede ver en los resultado, los valores de los errores de las neuronas van covergiendo pero para constantes muy altas. Inclusive, para las funciones. Esto se debe a la implementación de limites que se está manejando dentro de las funciones matemáticas. Consideramos que, entre varias configuraciones, la que maneja mejor consistencia es la úlitma que maneja `DenseNN([12, 5, 6, 2],['s', 's','r'], 0)`\n",
        "\n",
        "¿Qué aprendió de estos experimentos? (Se vale no haber aprendido nada, venimos a\n",
        "experimentar)\n",
        "\n",
        "> Dentro de estos experimentos se tuvo un entendimiento más amplio sobre el uso del gradiente del descenso. Además, se aprendió de como se debe mantener un buen registro y cálculo del error para el cálculo de backpropagation.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
