{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohpNe0Yaq99s"
      },
      "source": [
        "# Laboratorio 3: Regresión Lineal\n",
        "\n",
        "*   Marco Ferraro | B82957"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AmprGytuDd0G",
        "outputId": "8d6ad11d-9b1e-4168-9789-2af8b6f24f68"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-20fb7b43-4299-4143-88a9-ab0f552df622\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weight</th>\n",
              "      <th>Length1</th>\n",
              "      <th>Length2</th>\n",
              "      <th>Length3</th>\n",
              "      <th>Height</th>\n",
              "      <th>Width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.9</td>\n",
              "      <td>7.5</td>\n",
              "      <td>8.4</td>\n",
              "      <td>8.8</td>\n",
              "      <td>2.1120</td>\n",
              "      <td>1.4080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>32.0</td>\n",
              "      <td>12.5</td>\n",
              "      <td>13.7</td>\n",
              "      <td>14.7</td>\n",
              "      <td>3.5280</td>\n",
              "      <td>1.9992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>3.8240</td>\n",
              "      <td>2.4320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>51.5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.2</td>\n",
              "      <td>17.2</td>\n",
              "      <td>4.5924</td>\n",
              "      <td>2.6316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70.0</td>\n",
              "      <td>15.7</td>\n",
              "      <td>17.4</td>\n",
              "      <td>18.5</td>\n",
              "      <td>4.5880</td>\n",
              "      <td>2.9415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100.0</td>\n",
              "      <td>16.2</td>\n",
              "      <td>18.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>5.2224</td>\n",
              "      <td>3.3216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>78.0</td>\n",
              "      <td>16.8</td>\n",
              "      <td>18.7</td>\n",
              "      <td>19.4</td>\n",
              "      <td>5.1992</td>\n",
              "      <td>3.1234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>80.0</td>\n",
              "      <td>17.2</td>\n",
              "      <td>19.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>5.6358</td>\n",
              "      <td>3.0502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>85.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>19.6</td>\n",
              "      <td>20.8</td>\n",
              "      <td>5.1376</td>\n",
              "      <td>3.0368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>85.0</td>\n",
              "      <td>18.2</td>\n",
              "      <td>20.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>5.0820</td>\n",
              "      <td>2.7720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>110.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>22.5</td>\n",
              "      <td>5.6925</td>\n",
              "      <td>3.5550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>115.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>22.5</td>\n",
              "      <td>5.9175</td>\n",
              "      <td>3.3075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>125.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>22.5</td>\n",
              "      <td>5.6925</td>\n",
              "      <td>3.6675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>130.0</td>\n",
              "      <td>19.3</td>\n",
              "      <td>21.3</td>\n",
              "      <td>22.8</td>\n",
              "      <td>6.3840</td>\n",
              "      <td>3.5340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>120.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>6.1100</td>\n",
              "      <td>3.4075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>120.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>5.6400</td>\n",
              "      <td>3.5250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>130.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>6.1100</td>\n",
              "      <td>3.5250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>135.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>5.8750</td>\n",
              "      <td>3.5250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>110.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>5.5225</td>\n",
              "      <td>3.9950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>130.0</td>\n",
              "      <td>20.5</td>\n",
              "      <td>22.5</td>\n",
              "      <td>24.0</td>\n",
              "      <td>5.8560</td>\n",
              "      <td>3.6240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>150.0</td>\n",
              "      <td>20.5</td>\n",
              "      <td>22.5</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6.7920</td>\n",
              "      <td>3.6240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>145.0</td>\n",
              "      <td>20.7</td>\n",
              "      <td>22.7</td>\n",
              "      <td>24.2</td>\n",
              "      <td>5.9532</td>\n",
              "      <td>3.6300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>150.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>24.5</td>\n",
              "      <td>5.2185</td>\n",
              "      <td>3.6260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>170.0</td>\n",
              "      <td>21.5</td>\n",
              "      <td>23.5</td>\n",
              "      <td>25.0</td>\n",
              "      <td>6.2750</td>\n",
              "      <td>3.7250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>225.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>25.5</td>\n",
              "      <td>7.2930</td>\n",
              "      <td>3.7230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>145.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>25.5</td>\n",
              "      <td>6.3750</td>\n",
              "      <td>3.8250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>188.0</td>\n",
              "      <td>22.6</td>\n",
              "      <td>24.6</td>\n",
              "      <td>26.2</td>\n",
              "      <td>6.7334</td>\n",
              "      <td>4.1658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>180.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>26.5</td>\n",
              "      <td>6.4395</td>\n",
              "      <td>3.6835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>197.0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>25.6</td>\n",
              "      <td>27.0</td>\n",
              "      <td>6.5610</td>\n",
              "      <td>4.2390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>218.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>26.5</td>\n",
              "      <td>28.0</td>\n",
              "      <td>7.1680</td>\n",
              "      <td>4.1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>300.0</td>\n",
              "      <td>25.2</td>\n",
              "      <td>27.3</td>\n",
              "      <td>28.7</td>\n",
              "      <td>8.3230</td>\n",
              "      <td>5.1373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>260.0</td>\n",
              "      <td>25.4</td>\n",
              "      <td>27.5</td>\n",
              "      <td>28.9</td>\n",
              "      <td>7.1672</td>\n",
              "      <td>4.3350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>265.0</td>\n",
              "      <td>25.4</td>\n",
              "      <td>27.5</td>\n",
              "      <td>28.9</td>\n",
              "      <td>7.0516</td>\n",
              "      <td>4.3350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>250.0</td>\n",
              "      <td>25.4</td>\n",
              "      <td>27.5</td>\n",
              "      <td>28.9</td>\n",
              "      <td>7.2828</td>\n",
              "      <td>4.5662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>250.0</td>\n",
              "      <td>25.9</td>\n",
              "      <td>28.0</td>\n",
              "      <td>29.4</td>\n",
              "      <td>7.8204</td>\n",
              "      <td>4.2042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>300.0</td>\n",
              "      <td>26.9</td>\n",
              "      <td>28.7</td>\n",
              "      <td>30.1</td>\n",
              "      <td>7.5852</td>\n",
              "      <td>4.6354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>320.0</td>\n",
              "      <td>27.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>31.6</td>\n",
              "      <td>7.6156</td>\n",
              "      <td>4.7716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>514.0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>32.8</td>\n",
              "      <td>34.0</td>\n",
              "      <td>10.0300</td>\n",
              "      <td>6.0180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>556.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>34.5</td>\n",
              "      <td>36.5</td>\n",
              "      <td>10.2565</td>\n",
              "      <td>6.3875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>840.0</td>\n",
              "      <td>32.5</td>\n",
              "      <td>35.0</td>\n",
              "      <td>37.3</td>\n",
              "      <td>11.4884</td>\n",
              "      <td>7.7957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>685.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>36.5</td>\n",
              "      <td>39.0</td>\n",
              "      <td>10.8810</td>\n",
              "      <td>6.8640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>700.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>38.3</td>\n",
              "      <td>10.6091</td>\n",
              "      <td>6.7408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>700.0</td>\n",
              "      <td>34.5</td>\n",
              "      <td>37.0</td>\n",
              "      <td>39.4</td>\n",
              "      <td>10.8350</td>\n",
              "      <td>6.2646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>690.0</td>\n",
              "      <td>34.6</td>\n",
              "      <td>37.0</td>\n",
              "      <td>39.3</td>\n",
              "      <td>10.5717</td>\n",
              "      <td>6.3666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>900.0</td>\n",
              "      <td>36.5</td>\n",
              "      <td>39.0</td>\n",
              "      <td>41.4</td>\n",
              "      <td>11.1366</td>\n",
              "      <td>7.4934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>650.0</td>\n",
              "      <td>36.5</td>\n",
              "      <td>39.0</td>\n",
              "      <td>41.4</td>\n",
              "      <td>11.1366</td>\n",
              "      <td>6.0030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>820.0</td>\n",
              "      <td>36.6</td>\n",
              "      <td>39.0</td>\n",
              "      <td>41.3</td>\n",
              "      <td>12.4313</td>\n",
              "      <td>7.3514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>850.0</td>\n",
              "      <td>36.9</td>\n",
              "      <td>40.0</td>\n",
              "      <td>42.3</td>\n",
              "      <td>11.9286</td>\n",
              "      <td>7.1064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>900.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>42.5</td>\n",
              "      <td>11.7300</td>\n",
              "      <td>7.2250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1015.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>42.4</td>\n",
              "      <td>12.3808</td>\n",
              "      <td>7.4624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>820.0</td>\n",
              "      <td>37.1</td>\n",
              "      <td>40.0</td>\n",
              "      <td>42.5</td>\n",
              "      <td>11.1350</td>\n",
              "      <td>6.6300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>1100.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>44.6</td>\n",
              "      <td>12.8002</td>\n",
              "      <td>6.8684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>39.8</td>\n",
              "      <td>43.0</td>\n",
              "      <td>45.2</td>\n",
              "      <td>11.9328</td>\n",
              "      <td>7.2772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>1100.0</td>\n",
              "      <td>40.1</td>\n",
              "      <td>43.0</td>\n",
              "      <td>45.5</td>\n",
              "      <td>12.5125</td>\n",
              "      <td>7.4165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>40.2</td>\n",
              "      <td>43.5</td>\n",
              "      <td>46.0</td>\n",
              "      <td>12.6040</td>\n",
              "      <td>8.1420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>41.1</td>\n",
              "      <td>44.0</td>\n",
              "      <td>46.6</td>\n",
              "      <td>12.4888</td>\n",
              "      <td>7.5958</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20fb7b43-4299-4143-88a9-ab0f552df622')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20fb7b43-4299-4143-88a9-ab0f552df622 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20fb7b43-4299-4143-88a9-ab0f552df622');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Weight  Length1  Length2  Length3   Height   Width\n",
              "0      5.9      7.5      8.4      8.8   2.1120  1.4080\n",
              "1     32.0     12.5     13.7     14.7   3.5280  1.9992\n",
              "2     40.0     13.8     15.0     16.0   3.8240  2.4320\n",
              "3     51.5     15.0     16.2     17.2   4.5924  2.6316\n",
              "4     70.0     15.7     17.4     18.5   4.5880  2.9415\n",
              "5    100.0     16.2     18.0     19.2   5.2224  3.3216\n",
              "6     78.0     16.8     18.7     19.4   5.1992  3.1234\n",
              "7     80.0     17.2     19.0     20.2   5.6358  3.0502\n",
              "8     85.0     17.8     19.6     20.8   5.1376  3.0368\n",
              "9     85.0     18.2     20.0     21.0   5.0820  2.7720\n",
              "10   110.0     19.0     21.0     22.5   5.6925  3.5550\n",
              "11   115.0     19.0     21.0     22.5   5.9175  3.3075\n",
              "12   125.0     19.0     21.0     22.5   5.6925  3.6675\n",
              "13   130.0     19.3     21.3     22.8   6.3840  3.5340\n",
              "14   120.0     20.0     22.0     23.5   6.1100  3.4075\n",
              "15   120.0     20.0     22.0     23.5   5.6400  3.5250\n",
              "16   130.0     20.0     22.0     23.5   6.1100  3.5250\n",
              "17   135.0     20.0     22.0     23.5   5.8750  3.5250\n",
              "18   110.0     20.0     22.0     23.5   5.5225  3.9950\n",
              "19   130.0     20.5     22.5     24.0   5.8560  3.6240\n",
              "20   150.0     20.5     22.5     24.0   6.7920  3.6240\n",
              "21   145.0     20.7     22.7     24.2   5.9532  3.6300\n",
              "22   150.0     21.0     23.0     24.5   5.2185  3.6260\n",
              "23   170.0     21.5     23.5     25.0   6.2750  3.7250\n",
              "24   225.0     22.0     24.0     25.5   7.2930  3.7230\n",
              "25   145.0     22.0     24.0     25.5   6.3750  3.8250\n",
              "26   188.0     22.6     24.6     26.2   6.7334  4.1658\n",
              "27   180.0     23.0     25.0     26.5   6.4395  3.6835\n",
              "28   197.0     23.5     25.6     27.0   6.5610  4.2390\n",
              "29   218.0     25.0     26.5     28.0   7.1680  4.1440\n",
              "30   300.0     25.2     27.3     28.7   8.3230  5.1373\n",
              "31   260.0     25.4     27.5     28.9   7.1672  4.3350\n",
              "32   265.0     25.4     27.5     28.9   7.0516  4.3350\n",
              "33   250.0     25.4     27.5     28.9   7.2828  4.5662\n",
              "34   250.0     25.9     28.0     29.4   7.8204  4.2042\n",
              "35   300.0     26.9     28.7     30.1   7.5852  4.6354\n",
              "36   320.0     27.8     30.0     31.6   7.6156  4.7716\n",
              "37   514.0     30.5     32.8     34.0  10.0300  6.0180\n",
              "38   556.0     32.0     34.5     36.5  10.2565  6.3875\n",
              "39   840.0     32.5     35.0     37.3  11.4884  7.7957\n",
              "40   685.0     34.0     36.5     39.0  10.8810  6.8640\n",
              "41   700.0     34.0     36.0     38.3  10.6091  6.7408\n",
              "42   700.0     34.5     37.0     39.4  10.8350  6.2646\n",
              "43   690.0     34.6     37.0     39.3  10.5717  6.3666\n",
              "44   900.0     36.5     39.0     41.4  11.1366  7.4934\n",
              "45   650.0     36.5     39.0     41.4  11.1366  6.0030\n",
              "46   820.0     36.6     39.0     41.3  12.4313  7.3514\n",
              "47   850.0     36.9     40.0     42.3  11.9286  7.1064\n",
              "48   900.0     37.0     40.0     42.5  11.7300  7.2250\n",
              "49  1015.0     37.0     40.0     42.4  12.3808  7.4624\n",
              "50   820.0     37.1     40.0     42.5  11.1350  6.6300\n",
              "51  1100.0     39.0     42.0     44.6  12.8002  6.8684\n",
              "52  1000.0     39.8     43.0     45.2  11.9328  7.2772\n",
              "53  1100.0     40.1     43.0     45.5  12.5125  7.4165\n",
              "54  1000.0     40.2     43.5     46.0  12.6040  8.1420\n",
              "55  1000.0     41.1     44.0     46.6  12.4888  7.5958"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('fish_perch.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPCAKfN7x6zU",
        "outputId": "78b12002-e484-4788-c53a-48b22c123ba1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        5.9\n",
              "1       32.0\n",
              "2       40.0\n",
              "3       51.5\n",
              "4       70.0\n",
              "5      100.0\n",
              "6       78.0\n",
              "7       80.0\n",
              "8       85.0\n",
              "9       85.0\n",
              "10     110.0\n",
              "11     115.0\n",
              "12     125.0\n",
              "13     130.0\n",
              "14     120.0\n",
              "15     120.0\n",
              "16     130.0\n",
              "17     135.0\n",
              "18     110.0\n",
              "19     130.0\n",
              "20     150.0\n",
              "21     145.0\n",
              "22     150.0\n",
              "23     170.0\n",
              "24     225.0\n",
              "25     145.0\n",
              "26     188.0\n",
              "27     180.0\n",
              "28     197.0\n",
              "29     218.0\n",
              "30     300.0\n",
              "31     260.0\n",
              "32     265.0\n",
              "33     250.0\n",
              "34     250.0\n",
              "35     300.0\n",
              "36     320.0\n",
              "37     514.0\n",
              "38     556.0\n",
              "39     840.0\n",
              "40     685.0\n",
              "41     700.0\n",
              "42     700.0\n",
              "43     690.0\n",
              "44     900.0\n",
              "45     650.0\n",
              "46     820.0\n",
              "47     850.0\n",
              "48     900.0\n",
              "49    1015.0\n",
              "50     820.0\n",
              "51    1100.0\n",
              "52    1000.0\n",
              "53    1100.0\n",
              "54    1000.0\n",
              "55    1000.0\n",
              "Name: Weight, dtype: float64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_y = df['Weight'].copy()\n",
        "df_x = df.drop(columns=['Weight'])\n",
        "df_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEpq7radCweB"
      },
      "source": [
        "# 1. Función `MSE(y_true, y_predict)` \n",
        "Recibe dos objetos pd.Series que\n",
        "contienen los valores reales de un conjunto de datos y los valores estimados por un modelo. Calcule y retorne el error cuadrático medio de dicha predicción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6xkuOa9OClpv"
      },
      "outputs": [],
      "source": [
        "def MSE(y_true, y_predict, c=[], regularization='none', lbd=0):\n",
        "  sum = 0.0\n",
        "  n = len(y_true)\n",
        "  add_on = 0.0\n",
        "\n",
        "  if regularization == 'lasso' or regularization == 'l1':\n",
        "    sum_l1 = 0.0\n",
        "    for i in range(len(c)):\n",
        "      sum_l1 += abs(c[i])\n",
        "    add_on = lbd * sum_l1\n",
        "\n",
        "  elif regularization == 'ridge' or regularization == 'l2': \n",
        "    sum_l2 = 0.0\n",
        "    for i in range(len(c)):\n",
        "      sum_l2 += c[i]**2\n",
        "    add_on = lbd * sum_l2\n",
        "\n",
        "  for i in range(n):\n",
        "    sum += ((y_predict[i] - y_true[i])**2)\n",
        "  return float(sum / n) + add_on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7BSS0JDRkKW"
      },
      "source": [
        "# 2. Función `score(y_true, y_predict)`\n",
        "\n",
        "Recibe dos objetos pd.Series que\n",
        "contienen los valores reales de un conjunto de datos y los valores estimados por un\n",
        "modelo. Calcule y retorne el coeficiente de determinación (R^2) de dicha predicción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m_OvYzRtRiC7"
      },
      "outputs": [],
      "source": [
        "def score(y_true, y_predict):\n",
        "\n",
        "  y_true.to_numpy()\n",
        "  y_predict.to_numpy()\n",
        "\n",
        "  mean = y_true.mean()\n",
        "  first_sum = sum(((y_true - y_predict)**2))\n",
        "  second_sum = sum(((y_true - mean)**2))\n",
        "\n",
        "  return float((second_sum - first_sum) / second_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fsyBFgmpdG4K"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self):\n",
        "    self.c_vector = []\n",
        "    self.errors = []\n",
        "    self.past_dc = 0.0\n",
        "\n",
        "  def update_c(self, x, y, c, learning_rate, momentum=0):\n",
        "\n",
        "    a = (np.matmul(x, c) - y).T\n",
        "    b = np.matmul(a, x).T\n",
        "    dc = (2.0 / len(y)) * b\n",
        "\n",
        "    new_c = c - (learning_rate * (dc + (momentum * self.past_dc))) \n",
        "    self.past_dc = dc\n",
        "\n",
        "    return new_c\n",
        "\n",
        "  def predict(self, x, add_bias=False):\n",
        "    if (add_bias):\n",
        "       x.insert(0, \"bias\", 1.0, True)\n",
        "       x = x.to_numpy()\n",
        "    predict_y = []\n",
        "    for i in range(len(x)):\n",
        "      predict_y.append(np.dot(x[i], self.c_vector)[0])\n",
        "    \n",
        "    return pd.Series(predict_y)\n",
        "\n",
        "  def get_error_history(self):\n",
        "    return self.errors\n",
        "\n",
        "  def fit(self, x, y, max_epochs=1e5, threshold=1e-3, learning_rate=1e-5, momentum=0, decay=0, error='mse', regularization='none', lbd=0):\n",
        "    \n",
        "    ready = False\n",
        "    error = 0\n",
        "    iteration = 0\n",
        "    self.errors = []\n",
        "    \n",
        "    x.insert(0, \"bias\", 1.0, True)\n",
        "    x = x.to_numpy()\n",
        "    y = np.array([y.to_numpy()]).T\n",
        "    self.c_vector = np.array([random.sample(range(len(x[0])), len(x[0]))]).T\n",
        "\n",
        "    while not ready and iteration < max_epochs:\n",
        "      self.c_vector = self.update_c(x, y, self.c_vector, learning_rate, momentum=momentum)\n",
        "      new_error = MSE(y, np.matmul(x, self.c_vector), c=self.c_vector, regularization=regularization, lbd=lbd)\n",
        "      if abs(error - new_error) < threshold:\n",
        "        ready = True\n",
        "      error = new_error\n",
        "      self.errors.append(error)\n",
        "      learning_rate = learning_rate / (1 + decay)\n",
        "\n",
        "      iteration += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6aaeNwXGJoP"
      },
      "source": [
        "# 3 Funcionamiento del algoritmo \n",
        "Utilice el set de datos proveído para probar el funcionamiento de su algoritmo. Recuerde que el error debe reducirse en cada iteración del algoritmo (o llegar a un “zig-zag” producto de una tasa de aprendizaje muy elevada)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0GM8pnEt189",
        "outputId": "911b17ca-576e-4809-eab8-6848572c2ba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[126962.19681659035,\n",
              " 46489.85138762738,\n",
              " 45460.06570627436,\n",
              " 44465.1558930999,\n",
              " 43503.86625700528,\n",
              " 42574.98700212322,\n",
              " 41677.35254387599,\n",
              " 40809.839886881105,\n",
              " 39971.367062432226,\n",
              " 39160.89162336704,\n",
              " 38377.40919421352,\n",
              " 37619.95207458383,\n",
              " 36887.587893860014,\n",
              " 36179.41831528674,\n",
              " 35494.57778765642,\n",
              " 34832.2323428381,\n",
              " 34191.57843746596,\n",
              " 33571.84183716545,\n",
              " 32972.276541753854,\n",
              " 32392.16374991055,\n",
              " 31830.81086186687,\n",
              " 31287.550518718588,\n",
              " 30761.73967701612,\n",
              " 30252.758717336004,\n",
              " 29760.010585585947,\n",
              " 29282.919965840127,\n",
              " 28820.932483547575,\n",
              " 28373.513937996668,\n",
              " 27940.149562961964,\n",
              " 27520.343314497626,\n",
              " 27113.617184880528,\n",
              " 26719.510541741827,\n",
              " 26337.57949146285,\n",
              " 25967.39626594257,\n",
              " 25608.54863187942,\n",
              " 25260.639321739403,\n",
              " 24923.285485614666,\n",
              " 24596.11816320475,\n",
              " 24278.781775181385,\n",
              " 23970.9336332246,\n",
              " 23672.24346804535,\n",
              " 23382.392974732244,\n",
              " 23101.075374787626,\n",
              " 22827.994994239085,\n",
              " 22562.86685723601,\n",
              " 22305.416294563,\n",
              " 22055.378566521296]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr = LinearRegression()\n",
        "preds = lr.fit(df_x, df_y)\n",
        "\n",
        "errors = lr.get_error_history()\n",
        "reduced_errors = []\n",
        "jumps = 2150\n",
        "\n",
        "for i in range(0, len(errors), jumps): reduced_errors.append(errors[i])\n",
        "\n",
        "reduced_errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTcUn2dyMYJa"
      },
      "source": [
        "# 4 Split de datos \n",
        "Luego utilice el método train_test_split de la biblioteca sklearn.model_selection para separar un conjunto de datos en un conjunto de datos de entrenamiento y otro de prueba, utilice de semilla del split el número 21 (el método permite el parámetro opcional trandom_state para sembrar la aleatoriedad).\n",
        "\n",
        "\n",
        "\n",
        "> **Nota**: Para esta sección se hará el cálculo del r2 con scikit learn, ya que el modelo implementado genera `nan` en la primera resta\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAJCZADAMXpt",
        "outputId": "a069a5d1-d626-46b8-8d4a-21f4233e0ba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.798438051881311"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "X_fish_train, X_fish_test, y_fish_train, y_fish_test = train_test_split(df_x, df_y, test_size=0.3, random_state=21)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_fish_train, y_fish_train)\n",
        "\n",
        "pred = lr.predict(X_fish_test, add_bias=True)\n",
        "\n",
        "r2 = r2_score(y_fish_test, pred)\n",
        "r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0IW0GU2hh4D",
        "outputId": "e12aa1fe-f3f0-4786-d642-e987fb8fd2a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5111402628171239"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X_fish_train, X_fish_test, y_fish_train, y_fish_test = train_test_split(df_x, df_y, test_size=0.3, random_state=21)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_fish_train, y_fish_train, learning_rate=1e-6, decay=1e-3, regularization='l1', momentum=0.5, lbd=0.5)\n",
        "\n",
        "pred = lr.predict(X_fish_test, add_bias=True)\n",
        "\n",
        "r2 = r2_score(y_fish_test, pred)\n",
        "r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSrTQ4g8ho90",
        "outputId": "2d8f6a88-55c2-42a2-83bb-ab718ec9eaf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.49445357549938684"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X_fish_train, X_fish_test, y_fish_train, y_fish_test = train_test_split(df_x, df_y, test_size=0.3, random_state=21)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_fish_train, y_fish_train, learning_rate=1e-6, decay=1e-3, regularization='l2', momentum=0.5, lbd=0.5)\n",
        "\n",
        "pred = lr.predict(X_fish_test, add_bias=True)\n",
        "\n",
        "r2 = r2_score(y_fish_test, pred)\n",
        "r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHWMN0C6i-zr",
        "outputId": "b01b5075-876e-4e40-ab25-791110abc824"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8883128103547795"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X_fish_train, X_fish_test, y_fish_train, y_fish_test = train_test_split(df_x, df_y, test_size=0.3, random_state=21)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_fish_train, y_fish_train, learning_rate=1e-4, decay=1e-5, regularization='l1', momentum=0.7, lbd=0.7)\n",
        "\n",
        "pred = lr.predict(X_fish_test, add_bias=True)\n",
        "\n",
        "r2 = r2_score(y_fish_test, pred)\n",
        "r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqz8-5kwjCSd",
        "outputId": "04aac2cc-06ee-437f-a4fa-c38bf845eb23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7232117088627135"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X_fish_train, X_fish_test, y_fish_train, y_fish_test = train_test_split(df_x, df_y, test_size=0.3, random_state=21)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_fish_train, y_fish_train, learning_rate=1e-4, decay=1e-5, regularization='l2', momentum=0.7, lbd=0.7)\n",
        "\n",
        "pred = lr.predict(X_fish_test, add_bias=True)\n",
        "\n",
        "r2 = r2_score(y_fish_test, pred)\n",
        "r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaVcpMhCtF2k"
      },
      "source": [
        "## 4.1 ¿Cuál fue la combinación de parámetros que le proveyó el mejor resultado?\n",
        "\n",
        "\n",
        "> Viendo los resultados de las 4 pruebas que se lograron hacer, la combinación de parametros que brindó mejor resultado fue: \n",
        "```\n",
        "learning_rate=1e-4, decay=1e-5, regularization='l1', momentum=0.7, lbd=0.7\n",
        "```\n",
        "\n",
        "> Esto dió como resultado un `r2 score` de `0.8883647017635607`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRoW4NKEawH3"
      },
      "source": [
        "## 4.2 ¿Qué pasa si utiliza esa misma combinación pero cambia la semilla del train_test_split?  \n",
        "Pruebe con varias semillas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkM8oA8QapCi",
        "outputId": "e79ff3b5-c283-46be-d568-d6a7b413a5f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9356869264610277"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "X_fish_train, X_fish_test, y_fish_train, y_fish_test = train_test_split(df_x, df_y, test_size=0.3, random_state=53)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_fish_train, y_fish_train, learning_rate=1e-4, decay=1e-5, regularization='l1', momentum=0.7, lbd=0.7)\n",
        "\n",
        "pred = lr.predict(X_fish_test, add_bias=True)\n",
        "\n",
        "r2 = r2_score(y_fish_test, pred)\n",
        "r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbEvCRy7asPL",
        "outputId": "fb636c8f-8eed-4db4-bc14-9b78ca62d0b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9020686499545606"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "X_fish_train, X_fish_test, y_fish_train, y_fish_test = train_test_split(df_x, df_y, test_size=0.3, random_state=74)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_fish_train, y_fish_train, learning_rate=1e-4, decay=1e-5, regularization='l1', momentum=0.7, lbd=0.7)\n",
        "\n",
        "pred = lr.predict(X_fish_test, add_bias=True)\n",
        "\n",
        "r2 = r2_score(y_fish_test, pred)\n",
        "r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc-P5Lnlt5Td"
      },
      "source": [
        "## 4.3 Si pasa algo inusual: ¿Por qué cree que pasa esto?\n",
        "\n",
        "\n",
        "> Analizando los resultado, vemos que se generan `r2 score` mejores que los de las pruebas pasadas. Esto puede ser por la naturaleza de los datos, ya que al variar el set de entrenamiento el modelo puede que genere un mejor `fit` que los anteriores. \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ae115c8122ba000a648ea691cf086e95841d767d998fc7ed45e791bf48fd755e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
