{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "DIRECTORY = 'data'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    names = [n.decode('utf-8')\n",
    "             for n in unpickle(DIRECTORY + \"/batches.meta\")[b'label_names']]\n",
    "    x_train = None\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        data = unpickle(DIRECTORY + \"/data_batch_\" + str(i))\n",
    "        if i > 1:\n",
    "            x_train = np.append(x_train, data[b'data'], axis=0)\n",
    "        else:\n",
    "            x_train = data[b'data']\n",
    "        y_train += data[b'labels']\n",
    "\n",
    "    data = unpickle(DIRECTORY + \"/test_batch\")\n",
    "\n",
    "    x_test = data[b'data']\n",
    "    y_test = data[b'labels']\n",
    "\n",
    "    return names, x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor(tensor, perm=None):\n",
    "    if perm == None:\n",
    "        perm = (1, 2, 0)\n",
    "    plt.figure()\n",
    "    plt.imshow(tensor.permute(perm).cpu().numpy().astype(np.uint8))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names, x_train, y_train, x_test, y_test = load_data()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3, 32, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_train_tensors = torch.tensor(x_train).to(device)\n",
    "x_train_tensors = torch.reshape(x_train_tensors, [50000, 3, 32, 32])\n",
    "x_test_tensors = torch.tensor(x_train).to(device)\n",
    "x_test_tensors = torch.reshape(x_test_tensors, [50000, 3, 32, 32])\n",
    "\n",
    "x_test_tensors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "my_cnn = torch.nn.Conv2d(3, 1, (3, 3)).to(device)\n",
    "\n",
    "output = my_cnn(x_train_tensors.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "class Net(Module):\n",
    "  def __init__(self, num_channels, classes) -> None:\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    # conv_layer -> relu -> pool\n",
    "    self.conv_1 = Conv2d(in_channels=num_channels, out_channels=20, kernel_size=(5, 5))\n",
    "    self.relu_1 = ReLU()\n",
    "    self.maxpool_1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "    # second set\n",
    "    # conv -> relu -> pool\n",
    "    self.conv_2 = Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5))\n",
    "    self.relu_2 = ReLU()\n",
    "    self.maxpool_2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "    # initializa fully connected layer\n",
    "    self.fc_1 = Linear(in_features=800, out_features=500)\n",
    "    self.relu_3 = ReLU()\n",
    "\n",
    "    # initialize softmax classifier\n",
    "    self.fc_2 = Linear(in_features=500, out_features=classes)\n",
    "    self.log_soft_max = LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # the x data passes through our first set \n",
    "    # conv -> relu -> pool layers\n",
    "    x = self.conv_1(x)\n",
    "    print(x.shape)\n",
    "    x = self.relu_1(x)\n",
    "    x = self.maxpool_1(x)\n",
    "\n",
    "    # passes the output from previous layer to through the second\n",
    "    # conv -> relu -> pool\n",
    "\n",
    "    x = self.conv_2(x)\n",
    "    x = self.relu_2(x)\n",
    "    x = self.maxpool_2(x)\n",
    "\n",
    "    # flatten the output and passes it\n",
    "    # fc -> relu\n",
    "    x = flatten(x, 1)\n",
    "    x = self.fc_1(x)\n",
    "    x = self.relu_3(x)\n",
    "\n",
    "    # pass the output to our softmax to get our final output\n",
    "    x = self.fc_2(x)\n",
    "    return self.log_soft_max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (50x25 and 800x500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x_train_tensors)):\n\u001b[1;32m----> 8\u001b[0m   pred \u001b[39m=\u001b[39m model(x_train_tensors[i]\u001b[39m.\u001b[39;49mfloat())\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [24], line 50\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m# flatten the output and passes it\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m# fc -> relu\u001b[39;00m\n\u001b[0;32m     49\u001b[0m x \u001b[39m=\u001b[39m flatten(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc_1(x)\n\u001b[0;32m     51\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu_3(x)\n\u001b[0;32m     53\u001b[0m \u001b[39m# pass the output to our softmax to get our final output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50x25 and 800x500)"
     ]
    }
   ],
   "source": [
    "model = Net(num_channels=3, classes=len(names))\n",
    "model.to(device)\n",
    "\n",
    "for e in range(10):\n",
    "  model.train()\n",
    "\n",
    "  for i in range(len(x_train_tensors)):\n",
    "    pred = model(x_train_tensors[i].float())\n",
    "    # input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
